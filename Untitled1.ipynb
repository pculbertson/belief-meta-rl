{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from utils.buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n",
      "[ 0.11890073  0.39850731 -0.22082311 -1.06153696]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.make('SemisuperPendulumNoise-v0')\n",
    "if env.action_space.shape:\n",
    "    dim_actions = env.action_space.shape[0]\n",
    "    discrete_actions = False\n",
    "else:\n",
    "    dim_actions = env.action_space.n\n",
    "    discrete_actions = True\n",
    "dim_obs = env.observation_space.shape[0] if env.observation_space.shape else 1\n",
    "\n",
    "print(dim_actions, dim_obs)\n",
    "\n",
    "buffer_size = 10000\n",
    "if env.action_space.shape:\n",
    "    rb = ReplayBuffer(buffer_size,dim_obs,dim_actions)\n",
    "else:\n",
    "    rb = ReplayBuffer(buffer_size,dim_obs,1)\n",
    "num_episodes = 1\n",
    "iter = 0\n",
    "for i in range(num_episodes):\n",
    "    s, d = env.reset(), False\n",
    "    rb.new_episode()\n",
    "    while not d:\n",
    "        a = env.action_space.sample()\n",
    "        sp, r, d, _ = env.step(a)\n",
    "        rb.add_sample(s,a,r,sp,d)\n",
    "        if iter == 19:\n",
    "            print(s)\n",
    "        iter += 1\n",
    "        s = sp\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "seqs = rb.random_sequences(10,seq_length=20)\n",
    "print(seqs[1]['o'][-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(0,5))\n",
    "a[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2\n",
      "-200.0 tensor(102.9085, device='cuda:0', grad_fn=<AddBackward0>) tensor(168.1776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-3.9487574100494385 0.0011278721503913403\n",
      "-200.0 tensor(0.3496, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-4.6410722732543945 3.142240529996343e-06\n",
      "-200.0 tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-4.844973087310791 6.742162668160745e-07\n",
      "-200.0 tensor(0.0160, device='cuda:0', grad_fn=<AddBackward0>) tensor(2.6836e-05, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-5.566864490509033 5.324766334524611e-07\n",
      "-200.0 tensor(0.0167, device='cuda:0', grad_fn=<AddBackward0>) tensor(2.9134e-05, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-5.380547523498535 4.7209783815560513e-07\n",
      "-200.0 tensor(0.0312, device='cuda:0', grad_fn=<AddBackward0>) tensor(5.9359e-05, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-5.271930694580078 2.836083581314597e-07\n",
      "-200.0 tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>) tensor(2.5004e-05, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-5.390707015991211 2.250806545589512e-07\n",
      "-200.0 tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-5.457674026489258 1.6268558056253823e-07\n",
      "-200.0 tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>) tensor(4.0144e-05, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "-5.453503608703613 1.166583558642742e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cad9ee5151fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/belief-meta-rl/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# take a random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/belief-meta-rl/mb_policies.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mactions_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#print((states[:,:,i].shape,actions_input.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_state_rew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mbest_traj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/belief-meta-rl/mb_policies.py\u001b[0m in \u001b[0;36m_next_state_rew\u001b[0;34m(self, states, actions)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m\"\"\"helper function to unroll dynamics (batched)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mt_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mt_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_covs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mt_covs_clamped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_covs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_logvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_logvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(range(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.0000],\n",
       "        [0.0000, 0.2000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "dist = torch.distributions.MultivariateNormal(torch.tensor([0,0]),5*torch.eye(2))\n",
    "dist.precision_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[7.],\n",
       "         [7.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mats = torch.stack((torch.stack((torch.eye(2),2*torch.eye(2))),torch.stack((3*torch.eye(2),4*torch.eye(2)))))\n",
    "vecs = torch.stack((torch.stack((torch.ones(2),torch.ones(2))),torch.stack((torch.ones(2),torch.ones(2))))).unsqueeze(3)\n",
    "torch.sum(torch.matmul(mats,vecs),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.distributions import product_of_gaussians, gaussian_product_posterior\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.7647],\n",
      "         [-0.7647]],\n",
      "\n",
      "        [[-0.2500],\n",
      "         [-0.2500]]]), tensor([[[1.7000, 0.0000],\n",
      "         [0.0000, 1.7000]],\n",
      "\n",
      "        [[1.3333, 0.0000],\n",
      "         [0.0000, 1.3333]]]))\n",
      "(tensor([[[[-0.7647],\n",
      "          [-0.7647]]]]), tensor([[[[1.7000, 0.0000],\n",
      "          [0.0000, 1.7000]]]]))\n"
     ]
    }
   ],
   "source": [
    "covs = torch.stack((torch.stack((torch.eye(2),2*torch.eye(2),5*torch.eye(2))),torch.stack((2*torch.eye(2),2*torch.eye(2),3*torch.eye(2)))))\n",
    "means = torch.stack((torch.stack((torch.ones(2),-3*torch.ones(2),-4*torch.ones(2))),torch.stack((torch.ones(2),-3*torch.ones(2),2*torch.ones(2)))))\n",
    "precs = torch.inverse(covs)\n",
    "post = product_of_gaussians(means,precs)\n",
    "print(post)\n",
    "prior = product_of_gaussians(means[0,:2,:].view(1,2,2),precs[0,:2,:,:].view(1,2,2,2))\n",
    "posterior = gaussian_product_posterior(prior[0],prior[1],means[0,-1,:].view(1,2,1),precs[0,-1,:,:].view(1,1,2,2))\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, meta_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71811959  0.69591971 -0.66453942]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-439dc98860ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#sp, r, d, _ = env.step(env._goal-s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_goal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make('MetaPendulum-v0')#,randomize_tasks=True,n_tasks=10)\n",
    "#env.reset_task(1)\n",
    "s = env.reset()\n",
    "print(s)\n",
    "#sp, r, d, _ = env.step(env._goal-s)\n",
    "print(r)\n",
    "print(sp,env._goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MultivariateNormal' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-390838ea6719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#d2 = torch.distributions.MultivariateNormal(torch.zeros(2),10*torch.eye(2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#torch.distributions.kl.kl_divergence(d1,d2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'MultivariateNormal' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "d1 = torch.distributions.MultivariateNormal(50*torch.ones(50,2),20*torch.eye(2).expand(50,2,2))\n",
    "#d2 = torch.distributions.MultivariateNormal(torch.zeros(2),10*torch.eye(2))\n",
    "#torch.distributions.kl.kl_divergence(d1,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [2., 2., 2.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1.,2.]).view(2,1).expand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.9014])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covs= torch.stack((torch.eye(2),3*torch.eye(2)))\n",
    "means = torch.stack((torch.zeros(2),-1*torch.ones(2)))\n",
    "batch_mv_normal = torch.distributions.MultivariateNormal(means,covs)\n",
    "torch.distributions.kl.kl_divergence(batch_mv_normal,torch.distributions.MultivariateNormal(torch.zeros(2),torch.eye(2)).expand([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep_rew:  -1362.8793828882476\n",
      "traj errors:  tensor(611.2891, grad_fn=<AddBackward0>) tensor(8921.1660, grad_fn=<AddBackward0>)\n",
      "trans:  [-0.5262397   0.85033627 -2.23327395] tensor([ 0.3363, -0.3350,  0.4209], grad_fn=<MeanBackward1>)\n",
      "r:  -5.5566464282704064 tensor(0.2051, grad_fn=<MeanBackward1>)\n",
      "325 µs ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "314 µs ± 6.53 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "314 µs ± 7.49 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a6fb030fb986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mbatch_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_means\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mbatch_precs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_precs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'means, precs = product_of_gaussians(q_means.view(1,-1,latent_dim),q_precs.view(1,-1,latent_dim,latent_dim))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0mbatch_prod_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mbatch_prod_precs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cs330/env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/preston/Documents/school/cs330/env/lib/python3.7/site-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cs330/env/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cs330/env/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.2_2/Frameworks/Python.framework/Versions/3.7/lib/python3.7/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cs330/env/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cs330/project/utils/distributions.py\u001b[0m in \u001b[0;36mproduct_of_gaussians\u001b[0;34m(means, prec_mats)\u001b[0m\n\u001b[1;32m     50\u001b[0m        inputs: means (of shape [,K,N]), and prec_mats (of shape [,K,N,N])\"\"\"\n\u001b[1;32m     51\u001b[0m     \u001b[0mprec_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec_mats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec_mats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym, torch, meta_env\n",
    "import numpy as np\n",
    "from utils.buffer import ReplayBuffer\n",
    "from belief_models import SingleEncoder, TransitionNet, RewardNet\n",
    "from utils.distributions import get_cov_mat, log_transition_probs, log_rew_probs, product_of_gaussians, gaussian_product_posterior\n",
    "from belief_policies import RandomShooting\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#setup environment\n",
    "env = gym.make('MetaPendulum-v0')\n",
    "if env.action_space.shape:\n",
    "    dim_actions = env.action_space.shape[0]\n",
    "    discrete_actions = False\n",
    "else:\n",
    "    dim_actions = env.action_space.n\n",
    "    discrete_actions = True\n",
    "dim_obs = env.observation_space.shape[0] if env.observation_space.shape else 1\n",
    "\n",
    "#setup buffer\n",
    "buffer_size = 10000\n",
    "if env.action_space.shape:\n",
    "    rb = ReplayBuffer(buffer_size,dim_obs,dim_actions)\n",
    "else:\n",
    "    rb = ReplayBuffer(buffer_size,dim_obs,1)\n",
    "\n",
    "#training hyperparameters\n",
    "num_epochs = 5000\n",
    "global_iters = 0\n",
    "num_train_steps = 50\n",
    "max_logvar = 10.\n",
    "state_noise = 1e-3\n",
    "rew_noise = 1e-3\n",
    "random_episodes=3\n",
    "max_ep_length = 300\n",
    "\n",
    "#model hyperparameters\n",
    "trans_cov_type='diag'\n",
    "trans_hs=200\n",
    "\n",
    "rew_hs=200\n",
    "\n",
    "encoder_type = 'single'\n",
    "encoder_cov_type='diag'\n",
    "encoder_hs = 200\n",
    "latent_dim=30\n",
    "latent_prior = torch.distributions.MultivariateNormal(torch.zeros(latent_dim),torch.eye(latent_dim))\n",
    "test_batch_size = 100\n",
    "free_nats = 30.\n",
    "code_type = 'resample'\n",
    "\n",
    "trans_net = TransitionNet(dim_obs,dim_actions,latent_dim,cov_type=trans_cov_type,hs=trans_hs).to(device)\n",
    "rew_net = RewardNet(dim_obs,dim_actions,latent_dim,hs=rew_hs).to(device)\n",
    "if encoder_type == 'single':\n",
    "    encoder = SingleEncoder(dim_obs,dim_actions,latent_dim,cov_type=encoder_cov_type,hs=encoder_hs)\n",
    "#TODO: code multi-step encoder [RNN]\n",
    "\n",
    "#training parameters\n",
    "t_learning_rate = 1e-3\n",
    "t_optimizer = torch.optim.Adam(trans_net.parameters(),lr=t_learning_rate)\n",
    "\n",
    "r_learning_rate = 1e-2\n",
    "r_optimizer = torch.optim.Adam(rew_net.parameters(),lr=r_learning_rate)\n",
    "\n",
    "q_learning_rate = 1e-3\n",
    "q_optimizer = torch.optim.Adam(encoder.parameters(),lr=q_learning_rate)\n",
    "\n",
    "batch_size = 50\n",
    "batch_length = 50\n",
    "\n",
    "#planner hyperparameters\n",
    "num_traj = 1000\n",
    "traj_length = 10\n",
    "num_iters = 5\n",
    "elite_frac = 0.1\n",
    "\n",
    "if env.action_space.shape:\n",
    "    init_action_dist = torch.distributions.MultivariateNormal(torch.zeros(dim_actions),torch.from_numpy((env.action_space.high-env.action_space.low)**2)*torch.eye(dim_actions))\n",
    "    action_dist = [init_action_dist]*(traj_length-1)\n",
    "else:\n",
    "    init_action_dist = torch.distributions.Categorical(logits=torch.ones(env.action_space.n))\n",
    "    action_dist = [init_action_dist]*(traj_length-1)\n",
    "    \n",
    "policy = RandomShooting(trans_net, rew_net, encoder, init_action_dist, num_traj, traj_length, dim_obs, dim_actions, latent_dim, trans_cov_type, False, encoder_cov_type, max_logvar, device, code_type, det=False)\n",
    "\n",
    "losses = np.array([])\n",
    "rewards = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    m, l = np.random.uniform(0.1,50.), np.random.uniform(0.5,10.)\n",
    "    env.set_params(m,l)\n",
    "    rb.new_episode()\n",
    "    if epoch > 0:\n",
    "        for step in range(num_train_steps):\n",
    "            samps = rb.random_sequences(batch_size,batch_length)\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            t_optimizer.zero_grad()\n",
    "            r_optimizer.zero_grad()\n",
    "            \n",
    "            kl_divs, log_ts, log_rs = torch.zeros(batch_size), torch.zeros(batch_size), torch.zeros(batch_size)\n",
    "        \n",
    "            batch_means, batch_precs, batch_prod_means, batch_prod_precs = [], [], [], []\n",
    "            for i in range(batch_size):\n",
    "                s,a_rb,r,sp = [torch.from_numpy(samps[i][k]).float() for k in ['o','a','r','op']]\n",
    "                if discrete_actions:\n",
    "                    a = torch.squeeze(torch.nn.functional.one_hot(torch.from_numpy(a_rb).long(),num_classes=dim_actions)).float()\n",
    "                else:\n",
    "                    a = a_rb\n",
    "                q_ins = torch.cat((s,a,r,sp),axis=1)\n",
    "                q_outs = encoder(q_ins)\n",
    "                q_means = q_outs[:,:latent_dim]\n",
    "                q_precs = torch.inverse(get_cov_mat(q_outs[:,latent_dim:],dim_obs,encoder_cov_type,device))\n",
    "                batch_means.append(q_means)\n",
    "                batch_precs.append(q_precs)\n",
    "                %timeit means, precs = product_of_gaussians(q_means.view(1,-1,latent_dim),q_precs.view(1,-1,latent_dim,latent_dim))\n",
    "                batch_prod_means.append(means)\n",
    "                batch_prod_precs.append(precs)\n",
    "                \n",
    "            means, precs = torch.stack(batch_prod_means), torch.stack(batch_prod_precs)\n",
    "            \n",
    "            thetas = torch.squeeze(means + torch.matmul(torch.inverse(precs),torch.randn_like(means)))#shape: [BxL]\n",
    "            for i in range(batch_size):\n",
    "                s,a_rb,r,sp = [torch.from_numpy(samps[i][k]).float() for k in ['o','a','r','op']]\n",
    "                if discrete_actions:\n",
    "                    a = torch.squeeze(torch.nn.functional.one_hot(torch.from_numpy(a_rb).long(),num_classes=dim_actions)).float()\n",
    "                else:\n",
    "                    a = a_rb\n",
    "                net_ins = torch.cat((s,a,thetas[i,:].expand(batch_length,latent_dim)),axis=1)\n",
    "                t_outs = trans_net(net_ins)\n",
    "                r_outs = rew_net(net_ins)\n",
    "                r_means, r_covs = r_outs[:,0], torch.clamp(r_outs[:,1],-max_logvar,max_logvar)\n",
    "                t_means, t_covs = t_outs[:,:dim_obs], torch.clamp(t_outs[:,dim_obs:],-max_logvar,max_logvar)\n",
    "                log_ts[i] = torch.sum(log_transition_probs(t_means,t_covs,sp,cov_type=trans_cov_type,device=device))\n",
    "                t_mse = torch.nn.MSELoss()(t_means,sp)\n",
    "                #log_rs[i] = torch.sum(log_rew_probs(r_means,r_covs,r))\n",
    "                log_rs[i] = -torch.sum(torch.nn.MSELoss()(r_outs[:,0],r))\n",
    "                q_dist = torch.distributions.MultivariateNormal(batch_means[i],precision_matrix=batch_precs[i])\n",
    "                kl_divs[i] = torch.sum(torch.distributions.kl.kl_divergence(q_dist,latent_prior.expand([batch_length])))\n",
    "            \n",
    "            loss = torch.mean(kl_divs - log_ts - log_rs)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            q_optimizer.step()\n",
    "            t_optimizer.step()\n",
    "            r_optimizer.step()\n",
    "        losses = np.append(losses,loss.cpu().detach().numpy())\n",
    "        print(torch.mean(kl_divs),torch.mean(log_ts),torch.mean(log_rs))\n",
    "        print('mse: ', t_mse)\n",
    "        print(loss)\n",
    "\n",
    "    s, d, ep_rew = env.reset(), False, 0.\n",
    "    dyn_error, rew_error = 0, 0\n",
    "    ep_step = 0\n",
    "    latent_mean, latent_prec = torch.zeros(latent_dim), torch.eye(latent_dim)\n",
    "    while not d and ep_step < max_ep_length:\n",
    "        \n",
    "        if epoch < random_episodes:\n",
    "            a = np.array(env.action_space.sample())\n",
    "        else:\n",
    "            a = policy.get_action(s)\n",
    "        \n",
    "        sp, r, d, _ = env.step(a) # take a random action\n",
    "        s_n = s+np.random.multivariate_normal(np.zeros(dim_obs),state_noise*np.eye(dim_obs))\n",
    "        sp_n = sp+np.random.multivariate_normal(np.zeros(dim_obs),state_noise*np.eye(dim_obs))\n",
    "        r_n = r+np.random.normal(0.,rew_noise)\n",
    "        rb.add_sample(s_n,a,r_n,sp_n,d)\n",
    "        \n",
    "        codes = torch.squeeze(torch.distributions.MultivariateNormal(latent_mean,precision_matrix=latent_prec).rsample([test_batch_size]))\n",
    "        s_torch = torch.from_numpy(np.array(s)).float().expand(test_batch_size,dim_obs)\n",
    "        a_torch = torch.from_numpy(np.array(a)).float().expand(test_batch_size,dim_actions)\n",
    "        net_ins = torch.cat((s_torch,a_torch,codes),axis=1)\n",
    "        \n",
    "        sp_hats = trans_net(net_ins)[:,:dim_obs]\n",
    "        r_hats = rew_net(net_ins)[:,0]\n",
    "        \n",
    "        t_err = torch.mean(torch.nn.MSELoss()(torch.from_numpy(sp).float().expand(test_batch_size,dim_obs),sp_hats))\n",
    "        r_err = torch.mean(torch.nn.MSELoss()(torch.from_numpy(np.array([r])).float().expand(test_batch_size,1),r_hats))\n",
    "        \n",
    "        dyn_error += t_err\n",
    "        rew_error += r_err\n",
    "        \n",
    "        q_in = torch.cat([torch.from_numpy(np.array(k)).float() for k in [s,a,[r],sp]])\n",
    "        q_out = encoder(q_in)\n",
    "        new_mean, new_prec = q_out[:latent_dim], torch.inverse(get_cov_mat(q_out[latent_dim:],dim_obs,encoder_cov_type,device))\n",
    "        latent_mean, latent_prec = gaussian_product_posterior(latent_mean.view(1,latent_dim),latent_prec.view(1,latent_dim,latent_dim),new_mean.view(1,latent_dim),new_prec.view(1,latent_dim,latent_dim))\n",
    "        \n",
    "        \n",
    "                    \n",
    "        ep_rew += r\n",
    "        global_iters += 1\n",
    "        ep_step += 1\n",
    "        s = sp\n",
    "    rewards.append(ep_rew)\n",
    "    print('ep_rew: ', ep_rew)\n",
    "    print('traj errors: ', dyn_error,rew_error)\n",
    "    print('trans: ', sp, torch.mean(sp_hats,0))\n",
    "    print('r: ', r, torch.mean(r_hats,0))\n",
    "    #print(t_err,r_err)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (1, 3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(range(0,2),range(2,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.matmul(torch.eye(30).expand(1000,30,30),torch.ones(1000,30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'latent_mean' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-56fa9fdb2290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mnew_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_prec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_cov_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_cov_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latent_mean, latent_prec = gaussian_product_posterior(latent_mean.view(1,latent_dim),latent_prec.view(1,latent_dim,latent_dim),new_mean.view(1,latent_dim),new_prec.view(1,latent_dim,latent_dim))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cs330/env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/preston/Documents/school/cs330/env/lib/python3.7/site-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cs330/env/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cs330/env/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/cs330/env/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'latent_mean' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import gym, torch, meta_env\n",
    "import numpy as np\n",
    "from utils.buffer import ReplayBuffer\n",
    "from belief_models import SingleEncoder, TransitionNet, RewardNet\n",
    "from utils.distributions import get_cov_mat, log_transition_probs, log_rew_probs, product_of_gaussians, gaussian_product_posterior\n",
    "from belief_policies import RandomShooting\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#setup environment\n",
    "env = gym.make('MetaPendulum-v0')\n",
    "if env.action_space.shape:\n",
    "    dim_actions = env.action_space.shape[0]\n",
    "    discrete_actions = False\n",
    "else:\n",
    "    dim_actions = env.action_space.n\n",
    "    discrete_actions = True\n",
    "dim_obs = env.observation_space.shape[0] if env.observation_space.shape else 1\n",
    "\n",
    "#setup buffer\n",
    "buffer_size = 10000\n",
    "if env.action_space.shape:\n",
    "    rb = ReplayBuffer(buffer_size,dim_obs,dim_actions)\n",
    "else:\n",
    "    rb = ReplayBuffer(buffer_size,dim_obs,1)\n",
    "\n",
    "#training hyperparameters\n",
    "num_epochs = 5000\n",
    "global_iters = 0\n",
    "num_train_steps = 50\n",
    "max_logvar = 10.\n",
    "state_noise = 1e-3\n",
    "rew_noise = 1e-3\n",
    "random_episodes=3\n",
    "max_ep_length = 300\n",
    "\n",
    "#model hyperparameters\n",
    "trans_cov_type='diag'\n",
    "trans_hs=200\n",
    "\n",
    "rew_hs=200\n",
    "\n",
    "encoder_type = 'single'\n",
    "encoder_cov_type='diag'\n",
    "encoder_hs = 200\n",
    "latent_dim=30\n",
    "latent_prior = torch.distributions.MultivariateNormal(torch.zeros(latent_dim),torch.eye(latent_dim))\n",
    "test_batch_size = 100\n",
    "free_nats = 30.\n",
    "code_type = 'resample'\n",
    "\n",
    "trans_net = TransitionNet(dim_obs,dim_actions,latent_dim,cov_type=trans_cov_type,hs=trans_hs).to(device)\n",
    "rew_net = RewardNet(dim_obs,dim_actions,latent_dim,hs=rew_hs).to(device)\n",
    "if encoder_type == 'single':\n",
    "    encoder = SingleEncoder(dim_obs,dim_actions,latent_dim,cov_type=encoder_cov_type,hs=encoder_hs)\n",
    "#TODO: code multi-step encoder [RNN]\n",
    "\n",
    "#training parameters\n",
    "t_learning_rate = 1e-3\n",
    "t_optimizer = torch.optim.Adam(trans_net.parameters(),lr=t_learning_rate)\n",
    "\n",
    "r_learning_rate = 1e-2\n",
    "r_optimizer = torch.optim.Adam(rew_net.parameters(),lr=r_learning_rate)\n",
    "\n",
    "q_learning_rate = 1e-3\n",
    "q_optimizer = torch.optim.Adam(encoder.parameters(),lr=q_learning_rate)\n",
    "\n",
    "batch_size = 50\n",
    "batch_length = 50\n",
    "\n",
    "#planner hyperparameters\n",
    "num_traj = 1000\n",
    "traj_length = 10\n",
    "num_iters = 5\n",
    "elite_frac = 0.1\n",
    "\n",
    "if env.action_space.shape:\n",
    "    init_action_dist = torch.distributions.MultivariateNormal(torch.zeros(dim_actions),torch.from_numpy((env.action_space.high-env.action_space.low)**2)*torch.eye(dim_actions))\n",
    "    action_dist = [init_action_dist]*(traj_length-1)\n",
    "else:\n",
    "    init_action_dist = torch.distributions.Categorical(logits=torch.ones(env.action_space.n))\n",
    "    action_dist = [init_action_dist]*(traj_length-1)\n",
    "    \n",
    "policy = RandomShooting(trans_net, rew_net, encoder, init_action_dist, num_traj, traj_length, dim_obs, dim_actions, latent_dim, trans_cov_type, False, encoder_cov_type, max_logvar, device, code_type, det=False)\n",
    "\n",
    "losses = np.array([])\n",
    "rewards = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    m, l = np.random.uniform(0.1,50.), np.random.uniform(0.5,10.)\n",
    "    env.set_params(m,l)\n",
    "    rb.new_episode()\n",
    "    if epoch > 0:\n",
    "        for step in range(num_train_steps):\n",
    "            samps = rb.random_sequences(batch_size,batch_length)\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            t_optimizer.zero_grad()\n",
    "            r_optimizer.zero_grad()\n",
    "            \n",
    "            kl_divs, log_ts, log_rs = torch.zeros(batch_size), torch.zeros(batch_size), torch.zeros(batch_size)\n",
    "        \n",
    "            batch_means, batch_precs, batch_prod_means, batch_prod_precs = [], [], [], []\n",
    "            for i in range(batch_size):\n",
    "                s,a_rb,r,sp = [torch.from_numpy(samps[i][k]).float() for k in ['o','a','r','op']]\n",
    "                if discrete_actions:\n",
    "                    a = torch.squeeze(torch.nn.functional.one_hot(torch.from_numpy(a_rb).long(),num_classes=dim_actions)).float()\n",
    "                else:\n",
    "                    a = a_rb\n",
    "                q_ins = torch.cat((s,a,r,sp),axis=1)\n",
    "                q_outs = encoder(q_ins)\n",
    "                q_means = q_outs[:,:latent_dim]\n",
    "                q_precs = torch.inverse(get_cov_mat(q_outs[:,latent_dim:],dim_obs,encoder_cov_type,device))\n",
    "                batch_means.append(q_means)\n",
    "                batch_precs.append(q_precs)\n",
    "                %timeit means, precs = product_of_gaussians(q_means.view(1,-1,latent_dim),q_precs.view(1,-1,latent_dim,latent_dim))\n",
    "                batch_prod_means.append(means)\n",
    "                batch_prod_precs.append(precs)\n",
    "                \n",
    "            means, precs = torch.stack(batch_prod_means), torch.stack(batch_prod_precs)\n",
    "            \n",
    "            thetas = torch.squeeze(means + torch.matmul(torch.inverse(precs),torch.randn_like(means)))#shape: [BxL]\n",
    "            for i in range(batch_size):\n",
    "                s,a_rb,r,sp = [torch.from_numpy(samps[i][k]).float() for k in ['o','a','r','op']]\n",
    "                if discrete_actions:\n",
    "                    a = torch.squeeze(torch.nn.functional.one_hot(torch.from_numpy(a_rb).long(),num_classes=dim_actions)).float()\n",
    "                else:\n",
    "                    a = a_rb\n",
    "                net_ins = torch.cat((s,a,thetas[i,:].expand(batch_length,latent_dim)),axis=1)\n",
    "                t_outs = trans_net(net_ins)\n",
    "                r_outs = rew_net(net_ins)\n",
    "                r_means, r_covs = r_outs[:,0], torch.clamp(r_outs[:,1],-max_logvar,max_logvar)\n",
    "                t_means, t_covs = t_outs[:,:dim_obs], torch.clamp(t_outs[:,dim_obs:],-max_logvar,max_logvar)\n",
    "                log_ts[i] = torch.sum(log_transition_probs(t_means,t_covs,sp,cov_type=trans_cov_type,device=device))\n",
    "                t_mse = torch.nn.MSELoss()(t_means,sp)\n",
    "                #log_rs[i] = torch.sum(log_rew_probs(r_means,r_covs,r))\n",
    "                log_rs[i] = -torch.sum(torch.nn.MSELoss()(r_outs[:,0],r))\n",
    "                q_dist = torch.distributions.MultivariateNormal(batch_means[i],precision_matrix=batch_precs[i])\n",
    "                kl_divs[i] = torch.sum(torch.distributions.kl.kl_divergence(q_dist,latent_prior.expand([batch_length])))\n",
    "            \n",
    "            loss = torch.mean(kl_divs - log_ts - log_rs)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            q_optimizer.step()\n",
    "            t_optimizer.step()\n",
    "            r_optimizer.step()\n",
    "        losses = np.append(losses,loss.cpu().detach().numpy())\n",
    "        print(torch.mean(kl_divs),torch.mean(log_ts),torch.mean(log_rs))\n",
    "        print('mse: ', t_mse)\n",
    "        print(loss)\n",
    "\n",
    "    s, d, ep_rew = env.reset(), False, 0.\n",
    "    dyn_error, rew_error = 0, 0\n",
    "    ep_step = 0\n",
    "    latent_mean, latent_prec = torch.zeros(latent_dim), torch.eye(latent_dim)\n",
    "    while not d and ep_step < max_ep_length:\n",
    "        \n",
    "        if epoch < random_episodes:\n",
    "            a = np.array(env.action_space.sample())\n",
    "        else:\n",
    "            a = policy.get_action(s)\n",
    "        \n",
    "        sp, r, d, _ = env.step(a) # take a random action\n",
    "        s_n = s+np.random.multivariate_normal(np.zeros(dim_obs),state_noise*np.eye(dim_obs))\n",
    "        sp_n = sp+np.random.multivariate_normal(np.zeros(dim_obs),state_noise*np.eye(dim_obs))\n",
    "        r_n = r+np.random.normal(0.,rew_noise)\n",
    "        rb.add_sample(s_n,a,r_n,sp_n,d)\n",
    "        \n",
    "        codes = torch.squeeze(torch.distributions.MultivariateNormal(latent_mean,precision_matrix=latent_prec).rsample([test_batch_size]))\n",
    "        s_torch = torch.from_numpy(np.array(s)).float().expand(test_batch_size,dim_obs)\n",
    "        a_torch = torch.from_numpy(np.array(a)).float().expand(test_batch_size,dim_actions)\n",
    "        net_ins = torch.cat((s_torch,a_torch,codes),axis=1)\n",
    "        \n",
    "        sp_hats = trans_net(net_ins)[:,:dim_obs]\n",
    "        r_hats = rew_net(net_ins)[:,0]\n",
    "        \n",
    "        t_err = torch.mean(torch.nn.MSELoss()(torch.from_numpy(sp).float().expand(test_batch_size,dim_obs),sp_hats))\n",
    "        r_err = torch.mean(torch.nn.MSELoss()(torch.from_numpy(np.array([r])).float().expand(test_batch_size,1),r_hats))\n",
    "        \n",
    "        dyn_error += t_err\n",
    "        rew_error += r_err\n",
    "        \n",
    "        q_in = torch.cat([torch.from_numpy(np.array(k)).float() for k in [s,a,[r],sp]])\n",
    "        q_out = encoder(q_in)\n",
    "        new_mean, new_prec = q_out[:latent_dim], torch.inverse(get_cov_mat(q_out[latent_dim:],dim_obs,encoder_cov_type,device))\n",
    "        %timeit latent_mean, latent_prec = gaussian_product_posterior(latent_mean.view(1,latent_dim),latent_prec.view(1,latent_dim,latent_dim),new_mean.view(1,latent_dim),new_prec.view(1,latent_dim,latent_dim))\n",
    "        \n",
    "        \n",
    "                    \n",
    "        ep_rew += r\n",
    "        global_iters += 1\n",
    "        ep_step += 1\n",
    "        s = sp\n",
    "    rewards.append(ep_rew)\n",
    "    print('ep_rew: ', ep_rew)\n",
    "    print('traj errors: ', dyn_error,rew_error)\n",
    "    print('trans: ', sp, torch.mean(sp_hats,0))\n",
    "    print('r: ', r, torch.mean(r_hats,0))\n",
    "    #print(t_err,r_err)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class TestLSTM(torch.nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers,batch_first=True):\n",
    "        super(TestLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.LSTM = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=batch_first)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(hidden_size,output_size)\n",
    "            \n",
    "    def forward(self, x, hidden):\n",
    "        x, hidden = self.LSTM(x,hidden)\n",
    "        x = self.linear(self.relu(x))\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = TestLSTM(1,10,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-6ecdda539c5c>, line 116)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-6ecdda539c5c>\"\u001b[0;36m, line \u001b[0;32m116\u001b[0m\n\u001b[0;31m    stepwise_means, stepwise_precs = torch.cat([a]) for t in range(batch_length)],axis=1)\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gym, torch, meta_env\n",
    "import numpy as np\n",
    "from utils.buffer import ReplayBuffer\n",
    "from belief_models import LSTMEncoder, TransitionNet, RewardNet\n",
    "from utils.distributions import get_cov_mat, log_transition_probs, log_rew_probs, product_of_gaussians, gaussian_product_posterior\n",
    "from belief_policies import LSTMRandomShooting\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#setup environment\n",
    "env = gym.make('MetaPendulum-v0')\n",
    "if env.action_space.shape:\n",
    "    dim_actions = env.action_space.shape[0]\n",
    "    discrete_actions = False\n",
    "else:\n",
    "    dim_actions = env.action_space.n\n",
    "    discrete_actions = True\n",
    "dim_obs = env.observation_space.shape[0] if env.observation_space.shape else 1\n",
    "\n",
    "#setup buffer\n",
    "buffer_size = 10000\n",
    "if env.action_space.shape:\n",
    "    rb = ReplayBuffer(buffer_size,dim_obs,dim_actions)\n",
    "else:\n",
    "    rb = ReplayBuffer(buffer_size,dim_obs,1)\n",
    "\n",
    "#training hyperparameters\n",
    "num_epochs = 5000\n",
    "global_iters = 0\n",
    "num_train_steps = 20\n",
    "max_logvar = 10.\n",
    "state_noise = 1e-3\n",
    "rew_noise = 1e-3\n",
    "random_episodes=0\n",
    "max_ep_length = 200\n",
    "episode_repeat = 3\n",
    "\n",
    "#model hyperparameters\n",
    "trans_cov_type='diag'\n",
    "trans_hs=200\n",
    "\n",
    "rew_hs=200\n",
    "\n",
    "encoder_type = 'single'\n",
    "encoder_cov_type='diag'\n",
    "encoder_hs = 200\n",
    "encoder_num_layers = 1\n",
    "latent_dim=30\n",
    "latent_prior = torch.distributions.MultivariateNormal(torch.zeros(latent_dim),torch.eye(latent_dim))\n",
    "test_batch_size = 100\n",
    "\n",
    "trans_net = TransitionNet(dim_obs,dim_actions,latent_dim,cov_type=trans_cov_type,hs=trans_hs).to(device)\n",
    "rew_net = RewardNet(dim_obs,dim_actions,latent_dim,hs=rew_hs).to(device)\n",
    "encoder = LSTMEncoder(dim_obs,dim_actions,latent_dim,cov_type=encoder_cov_type,hs=encoder_hs,num_layers=encoder_num_layers)\n",
    "\n",
    "#training parameters\n",
    "t_learning_rate = 1e-3\n",
    "t_optimizer = torch.optim.Adam(trans_net.parameters(),lr=t_learning_rate)\n",
    "\n",
    "r_learning_rate = 1e-2\n",
    "r_optimizer = torch.optim.Adam(rew_net.parameters(),lr=r_learning_rate)\n",
    "\n",
    "q_learning_rate = 1e-3\n",
    "q_optimizer = torch.optim.Adam(encoder.parameters(),lr=q_learning_rate)\n",
    "\n",
    "#planner hyperparameters\n",
    "num_traj = 1000\n",
    "traj_length = 20\n",
    "num_iters = 5\n",
    "elite_frac = 0.1\n",
    "\n",
    "batch_size = 50\n",
    "batch_length = traj_length\n",
    "\n",
    "if env.action_space.shape:\n",
    "    init_action_dist = torch.distributions.MultivariateNormal(torch.zeros(dim_actions),torch.from_numpy((env.action_space.high-env.action_space.low)**2)*torch.eye(dim_actions))\n",
    "    action_dist = [init_action_dist]*(traj_length-1)\n",
    "else:\n",
    "    init_action_dist = torch.distributions.Categorical(logits=torch.ones(env.action_space.n))\n",
    "    action_dist = [init_action_dist]*(traj_length-1)\n",
    "    \n",
    "policy = LSTMRandomShooting(trans_net, rew_net, encoder, init_action_dist, num_traj, traj_length, dim_obs, dim_actions, latent_dim, trans_cov_type, False, encoder_cov_type, max_logvar, device, det=False)\n",
    "losses = np.array([])\n",
    "rewards = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch % episode_repeat == 0:\n",
    "        print('new params!')\n",
    "        m, l = np.random.uniform(0.1,50.), np.random.uniform(0.5,10.)\n",
    "        env.set_params(m,l)\n",
    "    rb.new_episode()\n",
    "    if epoch > 0:\n",
    "        for step in range(num_train_steps):\n",
    "            samps = rb.random_sequences(batch_size,batch_length)\n",
    "            \n",
    "            q_optimizer.zero_grad()\n",
    "            t_optimizer.zero_grad()\n",
    "            r_optimizer.zero_grad()\n",
    "            \n",
    "            kl_divs, log_ts, log_rs = torch.zeros(batch_size), torch.zeros(batch_size), torch.zeros(batch_size)\n",
    "        \n",
    "            #assuming all sequences are full-length\n",
    "            s,a_rb,r,sp = [torch.stack([torch.from_numpy(samp[k]).float() for samp in samps]) for k in ['o','a','r','op']]\n",
    "            \n",
    "            if discrete_actions:\n",
    "                a = torch.squeeze(torch.nn.functional.one_hot(torch.from_numpy(a_rb).long(),num_classes=dim_actions)).float()\n",
    "            else:\n",
    "                a = a_rb\n",
    "            \n",
    "            q_ins = torch.cat((s,a,r,sp),axis=2)\n",
    "            q_outs, _ = encoder(q_ins)\n",
    "            q_means = torch.squeeze(q_outs)[:,:,:latent_dim]\n",
    "            q_precs = torch.inverse(get_cov_mat(q_outs[:,:,latent_dim:],dim_obs,encoder_cov_type,device))\n",
    "                \n",
    "            post_means, post_precs = product_of_gaussians(q_means,q_precs)\n",
    "            stepwise_means, stepwise_precs = torch.cat([a]) for t in range(batch_length)],axis=1)\n",
    "            \n",
    "            thetas = (post_means + torch.matmul(torch.inverse(post_precs),torch.randn_like(post_means))).view(batch_size,1,-1)#shape: [BxL]\n",
    "            \n",
    "            net_ins = torch.cat((s,a,thetas.expand(batch_size,batch_length,latent_dim)),axis=2)\n",
    "            t_outs = trans_net(net_ins)\n",
    "            r_outs = rew_net(net_ins)\n",
    "            r_means, r_covs = r_outs[:,:,0], torch.clamp(r_outs[:,:,1],-max_logvar,max_logvar)\n",
    "            t_means, t_covs = t_outs[:,:,:dim_obs], torch.clamp(t_outs[:,:,dim_obs:],-max_logvar,max_logvar)\n",
    "            log_ts = torch.sum(log_transition_probs(t_means,t_covs,sp,cov_type=trans_cov_type,device=device),1)\n",
    "            t_mse = torch.nn.MSELoss()(t_means,sp)\n",
    "            log_rs = -torch.sum(torch.nn.MSELoss(reduction='none')(r_outs[:,:,0],torch.squeeze(r)),1)\n",
    "            \n",
    "            #fix this\n",
    "            prior_dist = torch.distributions.MultivariateNormal(q_means[:,:-1,:],q_covs[:,:-1,:,:])\n",
    "            post_dist = torch.distributions.MultivariateNormal(q_means[:,1:,:],q_covs[:,1:,:,:])\n",
    "            kl_divs = torch.sum(torch.distributions.kl.kl_divergence(post_dist,prior_dist))\n",
    "            \n",
    "            loss = torch.mean(kl_divs - log_ts - log_rs)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            q_optimizer.step()\n",
    "            t_optimizer.step()\n",
    "            r_optimizer.step()\n",
    "            print(step)\n",
    "        losses = np.append(losses,loss.cpu().detach().numpy())\n",
    "        print(torch.mean(kl_divs),torch.mean(log_ts),torch.mean(log_rs))\n",
    "        print('mse: ', t_mse)\n",
    "        print(loss)\n",
    "\n",
    "    s, d, ep_rew = env.reset(), False, 0.\n",
    "    dyn_error, rew_error = 0, 0\n",
    "    ep_step = 0\n",
    "    latent_mean, latent_cov = torch.zeros(latent_dim), torch.eye(latent_dim)\n",
    "    hidden = None\n",
    "    while not d and ep_step < max_ep_length:\n",
    "        \n",
    "        if epoch < random_episodes or epoch % episode_repeat == 0:\n",
    "            a = np.array(env.action_space.sample())\n",
    "        else:\n",
    "            if hidden is None:\n",
    "                a = policy.get_action(s,hidden)\n",
    "            else:\n",
    "                a = policy.get_action(s,[h.expand(encoder_num_layers,num_traj,encoder_hs) for h in hidden])\n",
    "        \n",
    "        sp, r, d, _ = env.step(a) # take a random action\n",
    "        s_n = s+np.random.multivariate_normal(np.zeros(dim_obs),state_noise*np.eye(dim_obs))\n",
    "        sp_n = sp+np.random.multivariate_normal(np.zeros(dim_obs),state_noise*np.eye(dim_obs))\n",
    "        r_n = r+np.random.normal(0.,rew_noise)\n",
    "        rb.add_sample(s_n,a,r_n,sp_n,d)\n",
    "        \n",
    "        codes = torch.squeeze(torch.distributions.MultivariateNormal(latent_mean,latent_cov).rsample([test_batch_size]))\n",
    "        s_torch = torch.from_numpy(np.array(s)).float().expand(test_batch_size,dim_obs)\n",
    "        a_torch = torch.from_numpy(np.array(a)).float().expand(test_batch_size,dim_actions)\n",
    "        net_ins = torch.cat((s_torch,a_torch,codes),axis=1)\n",
    "        \n",
    "        sp_hats = trans_net(net_ins)[:,:dim_obs]\n",
    "        r_hats = rew_net(net_ins)[:,0]\n",
    "        \n",
    "        t_err = torch.mean(torch.nn.MSELoss()(torch.from_numpy(sp).float().expand(test_batch_size,dim_obs),sp_hats))\n",
    "        r_err = torch.mean(torch.nn.MSELoss()(torch.from_numpy(np.array([r])).float().expand(test_batch_size,1),r_hats))\n",
    "        \n",
    "        dyn_error += t_err\n",
    "        rew_error += r_err\n",
    "        \n",
    "        q_in = torch.cat([torch.from_numpy(np.array(k)).float() for k in [s,a,[r],sp]]).view(1,1,-1)\n",
    "        q_out, hidden = encoder(q_in,hidden)\n",
    "        latent_mean, latent_cov = torch.squeeze(q_out)[:latent_dim], get_cov_mat(torch.squeeze(q_out)[latent_dim:],dim_obs,encoder_cov_type,device)        \n",
    "        \n",
    "        if ep_step == 5:\n",
    "            print('trans: ', sp, torch.mean(sp_hats,0))\n",
    "            print('r: ', r, torch.mean(r_hats,0))    \n",
    "        \n",
    "        ep_rew += r\n",
    "        global_iters += 1\n",
    "        ep_step += 1\n",
    "        s = sp\n",
    "    rewards.append(ep_rew)\n",
    "    print('ep_rew: ', ep_rew)\n",
    "    print('traj errors: ', dyn_error,rew_error)\n",
    "    #print(t_err,r_err)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unzip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a4254eb9d53b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unzip' is not defined"
     ]
    }
   ],
   "source": [
    "zipped = [(0,1),(2,3)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs330",
   "language": "python",
   "name": "cs330"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
